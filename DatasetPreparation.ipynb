{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Analysis and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook describes the process of cleaning up the dataset from \n",
    "[this Repository](https://github.com/docmarionum1/haikurnn) to use it in my own Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 1678116 words and 143139 Haikus\n",
      "229089 words are unique, thats about 13.0%\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/haikus.csv\", \"r\") as infile:\n",
    "    data = infile.read()\n",
    "\n",
    "print(\"The dataset contains {} words and {} Haikus\".format(len(data.split()), len(data.split('\\n'))))\n",
    "print(\"{} words are unique, thats about {}%\".format(len(set(data.split())), \n",
    "                                                    round(100*len(set(data.split()))/len(data.split()), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's, just for fun, look at the ten most common and least common words in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ten most common words are: ['the', 'to', 'I', 'a', 'and', 'of', 'my', 'you', 'is', 'in']\n",
      "The ten least common words are: ['snow,sballas,3,3,5', 'snow,sballas,3,3,4', 'ejection,u', 'approached,and', 'geese,faintly', 'strong,love', 'loveliness.,gutenberg,6,8,6', 'has,broken', 'ephesus,img2poems,7,8,7', 'with,ur']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_counts = Counter(data.split())\n",
    "\n",
    "print(\"The ten most common words are: {}\".format(sorted(word_counts, key=word_counts.get, reverse=True)[:10]))\n",
    "print(\"The ten least common words are: {}\".format(sorted(word_counts, key=word_counts.get, reverse=True)[-10:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the most common words are about what you would expect from a text dataset, the least common words are almost\n",
    "exclusively typos or the haiku source/the Syllable counter(e.g \"img2poems,7,8,7\"). If we want to use this dataset efficiently, we will have to remove the sources and syllable data and fix these obnoxious missing spaces after a comma. By introducing an alphabet which defines all the valid characters, we can also get rid of any special characters like _&_ or _#_ in the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ten most common words are: ['<n>', 'the', 'i', 'to', 'a', 'and', 'you', 'of', 'my', 'is']\n",
      "The ten least common words are: ['queensborough', 'moguls', 'xenon', 'interventionists', 'fleeced', 'sheri', 'sherk', 'mossscalloped', 'hashimoto', 'untalented']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "alphabet = string.ascii_lowercase + \" ,\"\n",
    "\n",
    "def cleanline(line):\n",
    "    if line[-5:] == \"5,7,5\":\n",
    "        line = line.lower()\t#remove uppercase letters\n",
    "        line = \"\".join([char for char in line if char in alphabet])\t#remove invalid chars\n",
    "        line = \" <n> \".join(line.split(\",\")[:3])\t#only select the actual haiku part\n",
    "        line = \" \".join(line.split())\t#remove multiple whitespaces\n",
    "        line = line.strip()\n",
    "        return line\n",
    "\n",
    "with open(\"data/haikus.csv\", \"r\") as infile:\n",
    "    lines = [cleanline(line) for line in infile.readlines() if cleanline(line) is not None]\n",
    "\n",
    "#count word occurences again\n",
    "word_counts = Counter(\" \".join(lines).split())\n",
    "\n",
    "print(\"The ten most common words are: {}\".format(sorted(word_counts, key=word_counts.get, reverse=True)[:10]))\n",
    "print(\"The ten least common words are: {}\".format(sorted(word_counts, key=word_counts.get, reverse=True)[-10:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The typos seem to be mostly gone and the least common words also seem about right now. Lets see how that \n",
    "reduced our number of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cleaned dataset contains 42938 unique words\n"
     ]
    }
   ],
   "source": [
    "print(\"The cleaned dataset contains {} unique words\".format(len(word_counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "43k words is already far better than 230k but its still far beyond my Model's capabilities. Ideally, we want to\n",
    "shrink the number of unique words(tokens) to around 10k or lower to keep training times feasible. It quickly becomes apparent that we might not be able to use the entirety of the dataset.(Lets ignore Haikus that dont fit\n",
    "the 5-7-5 syllable criteria). Filtering out all the haikus that contain words whose number of occurences falls below a certain threshold should reduce the number of unique tokens while still keeping a reasonable number of Haikus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8042 unique Tokens left\n"
     ]
    }
   ],
   "source": [
    "#my occurence threshold is arbitrary, i only chose it because it results in around 8k unique tokens left\n",
    "min_occurences = 12\n",
    "common = []\n",
    "\n",
    "for line in lines:\n",
    "    for word in line.split():\n",
    "        if word_counts[word] < min_occurences:\n",
    "            break\n",
    "    else:\n",
    "        common.append(line + \" <eos>\\n\")\n",
    "lines = common\n",
    "print(\"{} unique Tokens left\".format(len(set(\" \".join(lines).split()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use a batch size larger than 1, the samples need to be padded to the same length. This is easier if the haikus all have more or less the same length. Since most of the Haikus probably already are pretty similar in length, it is possible to just delete Haikus with a length below/above a certain threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lengths = [len(line.split()) for line in lines]\n",
    "counter = Counter(lengths)\n",
    "occurences = [counter[key] for key in counter.keys()]\n",
    "plt.bar(x=list(counter.keys()), height=occurences)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the bar graph above, i chose 12 as the minimum length and 15 as the maximum length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_lines = []\n",
    "for line in lines:\n",
    "    if 12 <= len(line.split()) <= 15:\n",
    "        cut_lines.append(line)\n",
    "lines = cut_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic preprocessing is done now but you might want to add a profanity filter and check for swearing words in\n",
    "the dataset. After all, the vast majority of the Haikus comes from Twitter([#Twaiku](https://twitter.com/hashtag/Twaiku)), meaning that some of them are a bit ... 'special'. Note that checking every single Haiku for bad words takes quite a while. You might want to do this outside of a notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from profanity_filter import ProfanityFilter\n",
    "\n",
    "#swear filter\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "profanity_filter = ProfanityFilter(nlps={'en': nlp})\n",
    "nlp.add_pipe(profanity_filter.spacy_component, last=True)\n",
    "\n",
    "#my occurence threshold is arbitrary, i only chose it because it results in around 8k unique tokens left\n",
    "min_occurences = 12\n",
    "contains_rare = 0\n",
    "not_profane = []\n",
    "\n",
    "for line in lines:\n",
    "\tif not nlp(line)._.is_profane:\n",
    "\t\tnot_profane.append(line + \" <eos>\\n\")\n",
    "lines = not_profane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the final collection of all the Haikus we can save the to a file and train our model on them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/dataset_clean.txt\", \"w\") as outfile:\n",
    "    outfile.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
