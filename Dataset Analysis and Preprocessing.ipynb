{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Analysis and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook describes the process of cleaning up the dataset from \n",
    "[this Repository](https://github.com/docmarionum1/haikurnn) to use it in my own Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 1678110 words and 143138 Haikus\n",
      "229084 words are unique, thats about 13.0%\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/haikus.csv\", \"r\") as infile:\n",
    "    data = infile.read()\n",
    "\n",
    "print(\"The dataset contains {} words and {} Haikus\".format(len(data.split()), len(data.split('\\n'))))\n",
    "print(\"{} words are unique, thats about {}%\".format(len(set(data.split())), \n",
    "                                                    round(100*len(set(data.split()))/len(data.split()), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's, just for fun, look at the ten most common and least common words in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ten most common words are: ['the', 'to', 'I', 'a', 'and', 'of', 'my', 'you', 'is', 'in']\n",
      "The ten least common words are: ['snow,sballas,3,3,5', 'snow,sballas,3,3,4', 'ejection,u', 'approached,and', 'geese,faintly', 'strong,love', 'loveliness.,gutenberg,6,8,6', 'has,broken', 'ephesus,img2poems,7,8,7', 'with,ur']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_counts = Counter(data.split())\n",
    "\n",
    "print(\"The ten most common words are: {}\".format(sorted(word_counts, key=word_counts.get, reverse=True)[:10]))\n",
    "print(\"The ten least common words are: {}\".format(sorted(word_counts, key=word_counts.get, reverse=True)[-10:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the most common words are about what you would expect from a text dataset, the least common words are almost\n",
    "exclusively typos or the haiku source/the Syllable counter(e.g \"img2poems,7,8,7\"). If we want to use this dataset efficiently, we will have to remove the sources and syllable data and fix these obnoxious missing spaces after a comma. By introducing an alphabet which defines all the valid characters, we can also get rid of any special characters like _&_ or _#_ in the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ten most common words are: ['the', 'i', 'to', 'a', 'and', 'you', 'of', 'my', 'is', 'in']\n",
      "The ten least common words are: ['queensborough', 'moguls', 'xenon', 'interventionists', 'fleeced', 'sheri', 'sherk', 'mossscalloped', 'hashimoto', 'untalented']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "alphabet = string.ascii_lowercase + \" \"\n",
    "\n",
    "def cleanline(line):\n",
    "\tline = \" \".join(line.split(\",\")[:3])\t#only select the actual haiku part\n",
    "\tline = line.lower()\t#remove uppercase letters\n",
    "\tline = \"\".join([char for char in line if char in alphabet])\t#remove invalid chars\n",
    "\tline = \" \".join(line.split())\t#remove multiple whitespaces\n",
    "\tline = line.strip()\n",
    "\treturn line\n",
    "\n",
    "with open(\"haikus.csv\", \"r\") as infile:\n",
    "    lines = [cleanline(line) for line in infile.readlines()]\n",
    "\n",
    "#count word occurences again\n",
    "word_counts = Counter(\" \".join(lines).split())\n",
    "\n",
    "print(\"The ten most common words are: {}\".format(sorted(word_counts, key=word_counts.get, reverse=True)[:10]))\n",
    "print(\"The ten least common words are: {}\".format(sorted(word_counts, key=word_counts.get, reverse=True)[-10:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The typos seem to be mostly gone and the least common words also seem about right now. Lets see how that \n",
    "reduced our number of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cleaned dataset contains 42938 unique words\n"
     ]
    }
   ],
   "source": [
    "print(\"The cleaned dataset contains {} unique words\".format(len(word_counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "43k words is already far better than 230k but its still far beyond my Model's capabilities. Ideally, we want to\n",
    "shrink the number of unique words(tokens) to around 10k or lower to keep training times feasible. It quickly becomes apparent that we might not be able to use the entirety of the dataset.(Lets ignore Haikus that dont fit\n",
    "the 5-7-5 syllable criteria). Filtering out all the haikus that contain words whose number of occurences falls below a certain threshold should reduce the number of unique tokens while still keeping a reasonable number of Haikus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8211 unique Tokens left\n"
     ]
    }
   ],
   "source": [
    "#my occurence threshold is arbitrary, i only chose it because it results in around 8k unique tokens left\n",
    "min_occurences = 12\n",
    "contains_rare = 0\n",
    "result = []\n",
    "\n",
    "for line in lines:\n",
    "    for word in line.split():\n",
    "        if word_counts[word] < min_occurences:\n",
    "            contains_rare += 1\n",
    "            break\n",
    "    else:\n",
    "        result.append(line + \"\\n\")\n",
    "        \n",
    "print(\"{} unique Tokens left\".format(len(set(\" \".join(result).split()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic preprocessing is done now but you might want to add a profanity filter and check for swearing words in\n",
    "the dataset. After all, the vast majority of the Haikus comes from Twitter([#Twaiku](https://twitter.com/hashtag/Twaiku)), meaning that some of them are a bit ... 'special'. Note that checking every single Haiku for bad words takes quite a while. You might want to do this outside of a notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from profanity_filter import ProfanityFilter\n",
    "\n",
    "#swear filter\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "profanity_filter = ProfanityFilter(nlps={'en': nlp})\n",
    "nlp.add_pipe(profanity_filter.spacy_component, last=True)\n",
    "\n",
    "#my occurence threshold is arbitrary, i only chose it because it results in around 8k unique tokens left\n",
    "min_occurences = 12\n",
    "contains_rare = 0\n",
    "result = []\n",
    "\n",
    "for line in lines:\n",
    "\tfor word in line.split():\n",
    "\t\tif occurences[word] < min_occurences:\n",
    "\t\t\tcontains_rare += 1\n",
    "\t\t\tbreak\n",
    "\telse:\n",
    "\t\tif not nlp(line)._.is_profane:\n",
    "\t\t\tresult.append(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the final collection of all the Haikus we can save the to a file and train our model on them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/dataset_clean.txt\", \"w\") as outfile:\n",
    "    outfile.writelines(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
